# @package _global_

estimator:
  _target_: mutinfo.estimators.parametric.minde.GenerativeMIEstimator
  name: InfoSEDD-MLP
  variant: j
  is_parametric_marginal: false
  backbone_factory:
    _target_: mutinfo.estimators.parametric.model_infosedd.GenericMLPDiffuser
    _partial_: true
    hidden_dim: 1024
  optimizer_factory:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 1e-2
  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: tmlr
    name: run_${now:%Y%m%d_%H%M%S_%f}
    group: null
    job_type: null
    id: null
  trainer:
    _target_: pytorch_lightning.Trainer
    max_steps: 100001
    val_check_interval: 1000
    check_val_every_n_epoch: null
    limit_val_batches: 1.0
    devices: [0]
    accelerator: 'gpu'
    logger: ${estimator.logger}
    enable_progress_bar: true
    fast_dev_run: False
    callbacks:
      - _target_: pytorch_lightning.callbacks.ModelCheckpoint
        dirpath: checkpoints/${estimator.name}/mi=${processed.mutual_information}/${now:%Y%m%d_%H%M%S_%f}
        filename: epoch_{epoch:02d}
        every_n_epochs: 1
        save_top_k: -1  # Save all checkpoints
        save_last: true
  graph:
    _target_: mutinfo.estimators.parametric.graph_lib.Absorbing
    dim: 300
  noise:
    _target_: mutinfo.estimators.parametric.noise_lib.LogLinearNoise
  train_batch_size: 512
  estimate_batch_size: 512
  estimate_fraction: 0.5 # `null` for no splitting.
  sampling_eps: 0.001

parameters_counter:
  _target_: builtins.eval
  _args_: ["lambda estimator, x, y: sum(parameters.numel() for parameters in estimator.backbone_factory(x.shape, y.shape).parameters())"]