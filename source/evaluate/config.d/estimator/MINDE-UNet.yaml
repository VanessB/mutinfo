# @package _global_

estimator:
  _target_: mutinfo.estimators.parametric.minde.GenerativeMIEstimator
  name: MINDE-Conv2d
  variant: c
  ckpt_path: null # Path to checkpoint to resume from.
  backbone_factory:
    _target_: mutinfo.estimators.parametric.model.UNet
    _partial_: true
    hidden_dim: 32
    layers_per_block: 2
  optimizer_factory:
    _target_: torch.optim.Adam
    _partial_: true
    lr: 2e-4
  logger:
    _target_: pytorch_lightning.loggers.WandbLogger
    project: tmlr
    name: run_${now:%Y%m%d_%H%M%S_%f}
    group: null
    job_type: null
    id: null
  trainer:
    _target_: pytorch_lightning.Trainer
    max_steps: 500001
    val_check_interval: 5000
    check_val_every_n_epoch: null
    limit_val_batches: 1.0
    devices: [0]
    accelerator: 'gpu'
    logger: ${estimator.logger}
    enable_progress_bar: true
    fast_dev_run: False
    callbacks:
      - _target_: pytorch_lightning.callbacks.ModelCheckpoint
        dirpath: checkpoints/${estimator.name}/mi=${processed.mutual_information}/${now:%Y%m%d_%H%M%S_%f}
        filename: epoch_{epoch:02d}
        every_n_epochs: 1
        save_top_k: -1  # Save all checkpoints
        save_last: true
  sde:
    _target_: mutinfo.estimators.parametric.sde_lib.VP_SDE
    beta_min: 0.1
    beta_max: 20.0
    T: 1.0
    importance_sampling: true
    weight_s_functions: true
    minde_type: ${estimator.variant}
    device: "cuda"
  train_batch_size: 512
  estimate_batch_size: 512
  estimate_fraction: 0.5 # `null` for no splitting.
  sampling_eps: 0.001

estimator_key: MINDE-UNet/type=${estimator.variant}/hidden=${estimator.backbone_factory.hidden_dim}/layers_per_block=${estimator.backbone_factory.layers_per_block}

parameters_counter:
  _target_: builtins.eval
  _args_: ["lambda estimator, x, y: sum(parameters.numel() for parameters in estimator.backbone_factory(x.shape, y.shape).parameters())"]